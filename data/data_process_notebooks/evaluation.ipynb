{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/.pyenv/versions/3.9.17/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "\n",
    "def inference(MODEL, final_prompt, pipe=False):\n",
    "    # 데이터 불러온 뒤 멀티 프롬프트 적용\n",
    "    df = pd.read_csv('../notebooks/data/test_multi.csv')\n",
    "    new_df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    prompter = Prompter('multi')\n",
    "    lst = []\n",
    "    for i in new_df['instruction']:\n",
    "        txt = prompter.generate_prompt(i)\n",
    "        lst.append(txt)\n",
    "\n",
    "    df = pd.DataFrame(lst, columns = ['input'])\n",
    "    messages = df['input'].to_list()\n",
    "\n",
    "    # 모델 불러온 후 프롬프트화 된 질문에 대한 응답 생성\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    ).to(device=f\"cuda\", non_blocking=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model.eval()\n",
    "\n",
    "    def gen(x):\n",
    "        inputs = tokenizer(\n",
    "            x, \n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()} \n",
    "        gened = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=512,\n",
    "            early_stopping=True,\n",
    "            do_sample=True,\n",
    "            eos_token_id=2,\n",
    "        )\n",
    "        gened_list = gened[0].tolist()\n",
    "        try:\n",
    "            eos_token_position = gened_list.index(2)\n",
    "            gened_list = gened_list[:eos_token_position]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        return tokenizer.decode(gened_list)\n",
    "\n",
    "    lst = []\n",
    "    if pipe:\n",
    "        generator = pipeline(\"text-generation\", model=model, tokenizer = tokenizer, device=0) \n",
    "        for message in tqdm(messages):\n",
    "            # generate response\n",
    "            res = generator(message, max_length=512, do_sample=True, eos_token_id=2)\n",
    "            output = res[0]['generated_text']\n",
    "            lst.append(output)\n",
    "            print('##################################################################')\n",
    "    else:\n",
    "        for message in tqdm(messages):\n",
    "            lst.append(gen(message))\n",
    "            print('##################################################################')\n",
    "    print('응답 생성 끝!!')\n",
    "\n",
    "    # 생성된 응답과 질문을 함께 GPT-4에 복사-붙여넣기 할 프롬프트에 넣고 저장\n",
    "    final_lst = []\n",
    "    for res in lst:\n",
    "        instruction = '\\n'.join(res.split('\\n')[2:12])\n",
    "        response = res.split('### 응답:')[-1].strip()\n",
    "\n",
    "        a = final_prompt.format(instruction=instruction, response=response)\n",
    "        final_lst.append(a)\n",
    "    \n",
    "    df = pd.DataFrame(final_lst, columns = ['GPT4_prompt'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = \"\"\"두 사람 간의 대화가 주어집니다. 다음의 지시문(Instruction)을 받게 될 것입니다. 지시문은 이전 대화내용을 포함하며 현재 대화에 대한 응답(Response)이 제시됩니다.\n",
    "당신의 작업은 응답을 평가 단계에 따라 응답을 평가하는 것입니다. \n",
    "이 평가 기준을 꼼꼼히 읽고 이해하는 것이 중요합니다. 평가하는 동안 이 문서를 계속 열어두고 필요할 때 참조해 주세요. \n",
    "\n",
    "평가 기준:\n",
    "- 이해 가능성 (0 - 1): Instruction에 기반하여 Response를 이해 할 수 있나요?\n",
    "- 자연스러움 (1 - 5): 사람이 자연스럽게 말할 법한 Instruction 인가요?\n",
    "- 맥락 유지 (1 - 5): Instruction을 고려했을 때 Response가 맥락을 유지하나요?\n",
    "- 흥미롭기 (1 - 5): Response가 지루한가요, 아니면 흥미로운가요?\n",
    "- Instruction 사용 (0 - 1): Instruction에 기반하여 Response를 생성 했나요?\n",
    "- 공감 능력 (0 - 1): Response에 Instruction의 내용에 기반한 공감의 내용이 있나요?\n",
    "- 대화 유도 (0 - 1): Response에 질문을 포함하여 사용자의 대답을 자연스럽게 유도하고 있나요?\n",
    "- 전반적인 품질 (1 - 10): 위의 답변을 바탕으로 이 발언의 전반적인 품질에 대한 인상은 어떤가요?\n",
    "\n",
    "평가 단계:\n",
    "1. Instruction, 그리고 Response을 주의깊게 읽습니다.\n",
    "2. 위의 평가 기준에 따라 Response을 엄격하게 평가합니다.\n",
    "\n",
    "Instruction:\n",
    "{instruction}\n",
    "\n",
    "Response:\n",
    "{response}\n",
    "\n",
    "\n",
    "Result\n",
    "- 이해 가능성 (0 - 1):\n",
    "- 자연스러움 (1 - 5):\n",
    "- 맥락 유지 (1 - 5):\n",
    "- 흥미롭기 (1 - 5):\n",
    "- Instruction 사용 (0 - 1):\n",
    "- 공감 능력 (0 - 1)\n",
    "- 대화 유도 (0 - 1): \n",
    "- 전반적인 품질 (1 - 10):\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 멀티 KULLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.67s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 1/50 [00:03<02:55,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:12<05:22,  6.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:18<05:01,  6.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:25<05:00,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:28<03:58,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:31<03:21,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:36<03:24,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:41<03:16,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:46<03:24,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:54<03:50,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:59<03:39,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [01:01<02:54,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [01:03<02:19,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [01:07<02:16,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [01:10<02:05,  3.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [01:17<02:36,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [01:20<02:12,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [01:27<02:36,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [01:33<02:43,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [01:37<02:25,  4.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [01:44<02:41,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [01:53<03:03,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [02:00<03:04,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [02:06<02:45,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [02:11<02:31,  6.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [02:14<02:00,  5.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [02:24<02:30,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [02:30<02:25,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [02:32<01:47,  5.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [02:37<01:43,  5.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [02:42<01:33,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [02:48<01:38,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [02:55<01:37,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [03:00<01:27,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [03:08<01:36,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [03:13<01:22,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [03:20<01:19,  6.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [03:25<01:11,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [03:29<00:59,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [03:37<01:00,  6.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [03:41<00:49,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [03:47<00:44,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [04:18<01:33, 13.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [04:22<01:03, 10.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [04:23<00:38,  7.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [04:30<00:29,  7.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [04:33<00:18,  6.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [04:43<00:14,  7.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [04:47<00:06,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:53<00:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "응답 생성 끝!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = inference('nlpai-lab/kullm-polyglot-12.8b-v2', final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('../notebooks/evaluate_datas/KULLM_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "koalpaca_after = pd.read_csv('../notebooks/evaluate_datas/Koalpaca_after.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(koalpaca_after['output'][0].split('\\n')[5].split(':')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 멀티 Koalpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = inference('beomi/KoAlpaca-Polyglot-12.8B', final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('../notebooks/evaluate_data/Koalpaca_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../notebooks/evaluate_datas/Koalpaca_after.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_scores(eval_result: str) -> list:\n",
    "    score_list = re.findall(r': (\\d+\\.?\\d*)', eval_result)\n",
    "    return list(map(float, score_list))\n",
    "column_names = [\n",
    "    'understandable',\n",
    "    'natural',\n",
    "    'maintains_context',\n",
    "    'interesting',\n",
    "    'uses_knowledge',\n",
    "    'empathy',\n",
    "    'conversational',\n",
    "    'overall_quality'\n",
    "]\n",
    "df[column_names] = pd.DataFrame(df['output'].apply(get_scores).to_list(), index=df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "understandable       0.788\n",
       "natural              3.810\n",
       "maintains_context    3.800\n",
       "interesting          3.020\n",
       "uses_knowledge       0.750\n",
       "empathy              0.720\n",
       "conversational       0.500\n",
       "overall_quality      6.200\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_scores = df[column_names].mean()\n",
    "avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../notebooks/evaluate_datas/Koalpaca.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. KoVicuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/.pyenv/versions/3.9.17/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "\n",
    "def inference_kovicuna(final_prompt):\n",
    "    # 데이터 불러온 뒤 멀티 프롬프트 적용\n",
    "    multi_df = pd.read_csv('../notebooks/evaluate_datas/test_multi.csv')\n",
    "    single_df = pd.read_csv('../notebooks/evaluate_datas/test_single.csv')\n",
    "\n",
    "    # 멀티-턴 데이터 입력을 위한 프롬프트 붙이기(정보 명시적으로 추가해주기)\n",
    "    prompter = Prompter('multi')\n",
    "    lst = []\n",
    "    for i in multi_df['instruction']:\n",
    "        txt = prompter.generate_prompt(i)\n",
    "        lst.append(txt)\n",
    "\n",
    "    multi_inputs = pd.DataFrame(lst, columns = ['input'])['input'].to_list()\n",
    "    single_inputs = single_df['instruction'].to_list()\n",
    "\n",
    "    print('문장 준비 완료!')\n",
    "\n",
    "    # 모델 불러온 후 질문에 대한 응답 생성\n",
    "    print('모델 불러오는 중....')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"junelee/ko_vicuna_7b\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"junelee/ko_vicuna_7b\", unk_token=\"<unk>\",bos_token=\"<s>\", eos_token=\"</s>\")\n",
    "    model.eval()\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer = tokenizer, device=0, max_new_tokens = 512)\n",
    "    print('모델 준비 완료!!')\n",
    "\n",
    "    \n",
    "    print('멀티-턴 생성 시작')\n",
    "    lst = []\n",
    "    for message in tqdm(multi_inputs):\n",
    "        res = generator(message, do_sample=True, eos_token_id=2)\n",
    "        output = res[0]['generated_text']\n",
    "        lst.append(output)\n",
    "        print('##################################################################')\n",
    "    print('멀티-턴 응답 생성 끝!!')\n",
    "\n",
    "    # 생성된 응답과 질문을 함께 GPT-4에 복사-붙여넣기 할 프롬프트에 넣고 저장\n",
    "    final_lst = []\n",
    "    for i, res in enumerate(lst):\n",
    "        instruction = multi_df.loc[i, 'instruction']\n",
    "        response = res.split('### 응답:')[-1].strip()\n",
    "\n",
    "        a = final_prompt.format(instruction=instruction, response=response)\n",
    "        final_lst.append(a)\n",
    "    \n",
    "    multi = pd.DataFrame(final_lst, columns = ['GPT4_prompt'])\n",
    "\n",
    "\n",
    "    print('싱글-턴 생성 시작')\n",
    "    lst = []\n",
    "    for message in tqdm(single_inputs):\n",
    "        res = generator(message, do_sample=True, eos_token_id=2)\n",
    "        output = res[0]['generated_text']\n",
    "        lst.append(output)\n",
    "        print('##################################################################')\n",
    "    print('싱글-턴 응답 생성 끝!!')\n",
    "\n",
    "    # 생성된 응답과 질문을 함께 GPT-4에 복사-붙여넣기 할 프롬프트에 넣고 저장\n",
    "    final_lst = []\n",
    "    for i, res in enumerate(lst):\n",
    "        instruction = single_df.loc[i, 'instruction']\n",
    "        response = res.split('### 응답:')[-1].strip()\n",
    "\n",
    "        a = final_prompt.format(instruction=instruction, response=response)\n",
    "        final_lst.append(a)\n",
    "    \n",
    "    single = pd.DataFrame(final_lst, columns = ['GPT4_prompt'])\n",
    "\n",
    "    return multi, single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi, single = inference_kovicuna(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi.to_csv('../notebooks/evaluate_datas/Kovicuna_multi_eval.csv')\n",
    "single.to_csv('../notebooks/evaluate_datas/Kovicuna_single_eval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_multi = pd.read_csv('../notebooks/evaluate_datas/eval_0727_multi.csv', encoding='cp949')\n",
    "final_single = pd.read_csv('../notebooks/evaluate_datas/eval_0727_single.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "understandable       0.97\n",
       "natural              4.34\n",
       "maintains_context    4.82\n",
       "interesting          2.76\n",
       "uses_knowledge       0.97\n",
       "empathy              0.91\n",
       "conversational       0.82\n",
       "overall_quality      7.30\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 멀티\n",
    "import re\n",
    "\n",
    "def get_scores(eval_result: str) -> list:\n",
    "    score_list = re.findall(r': (\\d+\\.?\\d*)', eval_result)\n",
    "    return list(map(float, score_list))\n",
    "\n",
    "column_names = [\n",
    "    'understandable',\n",
    "    'natural',\n",
    "    'maintains_context',\n",
    "    'interesting',\n",
    "    'uses_knowledge',\n",
    "    'empathy',\n",
    "    'conversational',\n",
    "    'overall_quality'\n",
    "]\n",
    "final_single[column_names] = pd.DataFrame(final_single['output'].apply(get_scores).to_list(), index=final_single.index)\n",
    "\n",
    "avg_scores = final_single[column_names].mean()\n",
    "avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_multi.to_csv('../notebooks/evaluate_datas/final_multi.csv', index=False)\n",
    "final_single.to_csv('../notebooks/evaluate_datas/final_single.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalproject-n-IakgBe-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

import torch
import gradio as gr
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, PeftConfig

def load_model():
    """Load the local Hugging Face model."""
    MODEL = "lora_merged_model" 
    model = AutoModelForCausalLM.from_pretrained(
        MODEL,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    ).to(device=f"cuda", non_blocking=True)
    tokenizer = AutoTokenizer.from_pretrained(MODEL)
    model.eval()
    return model, tokenizer

model, tokenizer = load_model()

pipe = pipeline(
    'text-generation',
    model = model,
    tokenizer = tokenizer,
    device=0
)

def answer(state, state_chatbot, text):
    messages = state + [{'role' : 'ëª…ë ¹ì–´', 'content' : text}]

    conversation_history = '\n'.join(
        [f"### {msg['role']}:\n{msg['content']}" for msg in messages]
    )
    print(conversation_history)

    ans = pipe(
        conversation_history + '\n### ì‘ë‹µ:',
        do_sample = True,
        max_new_tokens = 512,
        temperature = 0.7,
        top_p = 0.9,
        return_full_text = False,
        eos_token_id = 2,
    )

    msg = ans[0]['generated_text']
    if '###' in msg:
        msg = msg.split('###')[0]
    
    new_state = [{'role' : 'ì´ì „ ëª…ë ¹ì–´', 'content' : text},
                 {'role' : 'ì´ì „ ì‘ë‹µ', 'content' : msg}]
    
    state = state + new_state
    state_chatbot = state_chatbot + [[text, msg]]

    print('state : ', state)
    print('state_chatbot : ', state_chatbot)

    return state, state_chatbot, state_chatbot


with gr.Blocks(css="#chatbot .overflow-y-auto{height:750px}") as demo:
    state = gr.State(
        [
            {
                'role' : 'ë§¥ë½',
                'content' : 'ê³µê°ì„ ì˜í•˜ëŠ” ì±—ë´‡ "ë‚´ë§ˆë¦¬" ì…ë‹ˆë‹¤. ì‚¬ëŒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤ë‹ˆë‹¤.'
            },
            {
                'role' : 'ë§¥ë½',
                'content' : 'ì œ ëª©í‘œëŠ” ìœ ìš©í•˜ê³  ì¹œì ˆí•˜ë©° ì¬ë¯¸ìˆëŠ” ì±—ë´‡ì´ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì €ì—ê²Œ ì¡°ì–¸ì„ êµ¬í•˜ê±°ë‚˜ ë‹µë³€ì„ ìš”ì²­í•˜ê±°ë‚˜ ë¬´ìŠ¨ ì¼ì´ë“  ì´ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
            },
            {
                "role": "ëª…ë ¹",
                "content": "ì•„ë˜ëŠ” ì¼ìƒì  ëŒ€í™”ì—ì„œì˜ ì§ˆë¬¸ì…ë‹ˆë‹¤. ìµœëŒ€ 3ë¬¸ì¥ì˜ ê°„ê²°í•˜ê³  ì¹œì ˆí•œ ì‘ë‹µì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
            },
        ]
    )
    state_chatbot = gr.State([])
    
    with gr.Row():
        gr.Markdown("<h3><center>ê³µê°ëŠ¥ë ¥ ê°œì©ŒëŠ” ë¶€ë•ì´ì™€ì˜ ëŒ€í™”</center></h3>")

    chatbot = gr.Chatbot()

    with gr.Row():
        txt = gr.Textbox(
            label="ì–´ë–¤ ê²ƒì„ ë¬»ê³  ì‹¶ë‚˜ìš”?",
            placeholder="ì° Fì¸ ë¶€ë•ì´ì—ê²Œ ì–´ë–¤ ë§ì´ë“  ììœ ë¡­ê²Œ í•´ì£¼ì„¸ìš”!",
            lines=1,
        ).style(container = False)

    gr.HTML("Demo application of a Polyglot-ko, Huggingface")
    gr.HTML(
        "<center>Powered by <a href='https://github.com/hwchase17/langchain'>LangChain ğŸ¦œï¸ğŸ”—</a></center>"
    )

    txt.submit(answer, [state, state_chatbot, txt], [state, state_chatbot, chatbot])
    txt.submit(lambda: "", None, txt)

demo.launch(debug=True, share=True)

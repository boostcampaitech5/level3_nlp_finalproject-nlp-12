{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adapter.bin íŒŒì¼ë¡œ ë¶ˆëŸ¬ì™€ì„œ í™•ì¸í•´ë³´ê¸°\n",
    "- ì´ê±´ ì´ì œ ì•ˆí•´ë´ë„ ë  ë“¯ í•©ë‹ˆë‹¤.ê¶ê¸ˆí•œ ì‚¬ëŒë§Œ..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/final did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:13<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from utils.prompter import Prompter\n",
    "test_prompte_name = \"multi\"\n",
    "prompter = Prompter(f\"{test_prompte_name}\")\n",
    "\n",
    "start = time.time()\n",
    "peft_model_id = \"./model/lora_finetuned/new_2/\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, \n",
    "                                             quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.generate() inference ë°©ë²•\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "[ì´ì „ ëŒ€í™”]\n",
      "\n",
      "\n",
      "[í˜„ì¬ ëŒ€í™”]\n",
      "input: ë‚˜ëŠ” ì»¤ì„œ ì—°ê¸°ë¥¼ í•˜ëŠ” ë°°ìš°ê°€ ë ê±°ì•¼! ê·¸ê²Œ ë‚˜ì˜ ê¿ˆì´ì•¼\n",
      "\n",
      "response: ì—°ê¸°í•˜ëŠ” ë°°ìš°ê°€ ë˜ê³  ì‹¶ì€ ë§ˆìŒì€ ì•Œê² ì–´! ë„ˆì˜ ê¿ˆì„ ì´ë£¨ê¸° ìœ„í•´ì„œëŠ” ë„ˆì˜ ê¿ˆì„ ë” ìì„¸íˆ ë§í•´ì¤˜! ë‚´ê°€ ë” ì¢‹ì€ ë°©ë²•ì„ ì•Œë ¤ì¤„ê»˜![ì¶”ê°€ ì§ˆë¬¸] ë‹¬í•œ[ì¶”ê°€ ì§ˆë¬¸](1): ë„ˆëŠ”(2): 1. ë„ˆëŠ”! ë„ˆì˜ ê¿ˆì„ ì´ë£¨ê¸° ìœ„í•´ ë¬´ì—‡ì„ í•˜ê³  ìˆë‹ˆ! (3.ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•˜ê¸° ìœ„í•´ ë” ì—´ì‹¬íˆ ë…¸ë ¥í•˜ê³  ìˆì–´! ë„ˆì˜ ê¿ˆì„ ì´ë£¨ê¸° ìœ„í•´ ë„ˆì˜ í•˜ë£¨ë¥¼ ë” ë³´ì—¬ì¤˜!ë„ˆì˜ ê¿ˆì´ ë” ê¶ê¸ˆí•´! ë„ˆì˜ ê¿ˆì„ ì´ë£¨ê¸°\n",
      "ê±¸ë¦° ì‹œê°„ : 15ì´ˆ\n",
      "##################################################################\n",
      "model.generate() inference ë°©ë²•\n",
      "ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "[ì´ì „ ëŒ€í™”]\n",
      "\n",
      "\n",
      "[í˜„ì¬ ëŒ€í™”]\n",
      "input: ì´ì œ ì¼ ê·¸ë§Œí•˜ê³  ì§‘ì—ê°€ì„œ ì‰¬ê³ ì‹¶ì–´, ë„˜ í˜ë“¤ì–´ ã… ã… \n",
      "\n",
      "response: í˜ë“¤ê³  í”¼ê³¤í•˜ê³  ì§€ì¹œ ë ¨(\")(DOM) ANDROIUDELIV(\"DOM\")DOMINE(ALLOWEEKLY)ANDRE(ALLOWEEKLIVE)DOMINE(ANDRELLALIVE)DOMINE(ANDRELLALLOWEEIVEDOMINE(ANDRELLANDIVE(S340-DOMINE(ANDRE\n",
      "ê±¸ë¦° ì‹œê°„ : 14ì´ˆ\n",
      "##################################################################\n"
     ]
    }
   ],
   "source": [
    "def gen(x):\n",
    "    inputs = tokenizer(\n",
    "        x, \n",
    "        return_tensors='pt', \n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()} # move to same device as model\n",
    "    gened = model.generate(\n",
    "        **inputs,\n",
    "        min_new_tokens=30,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.5,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "        # no_repeat_ngram_size=2,\n",
    "        # repetition_penalty=1.05,\n",
    "        # top_k=20,\n",
    "        # top_p=0.95,\n",
    "        # typical_p=0.9\n",
    "    )\n",
    "    \n",
    "    gened_list = gened[0].tolist()\n",
    "    \n",
    "    try:\n",
    "        eos_token_position = gened_list.index(2)\n",
    "        gened_list = gened_list[:eos_token_position]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    print(tokenizer.decode(gened_list))\n",
    "\n",
    "messages = [\n",
    "            # 'ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§‘ì•„ì„œ ê¸°ë¶„ì´ ì¢‹ì•„ìš” !', \n",
    "            # 'ë‚´ì¼ ì‹œí—˜ì¸ë° ë„ˆë¬´ ë–¨ë ¤ì„œ ì ì´ ì•ˆì™€ìš”. ì–´ë–¡í•˜ì£ ?',\n",
    "            'ë‚˜ëŠ” ì»¤ì„œ ì—°ê¸°ë¥¼ í•˜ëŠ” ë°°ìš°ê°€ ë ê±°ì•¼! ê·¸ê²Œ ë‚˜ì˜ ê¿ˆì´ì•¼',\n",
    "            'ì´ì œ ì¼ ê·¸ë§Œí•˜ê³  ì§‘ì—ê°€ì„œ ì‰¬ê³ ì‹¶ì–´, ë„˜ í˜ë“¤ì–´ ã… ã… ',\n",
    "            # 'ì¹œêµ¬í•˜ê³  ì‹¸ì› ëŠ”ë° ì–´ë–»ê²Œ í™”í•´ë¥¼ í•˜ë©´ ì¢‹ì„ì§€ ëª¨ë¥´ê² ì–´'\n",
    "            ]\n",
    "for message in messages:\n",
    "    prompt = prompter.generate_prompt(message)\n",
    "\n",
    "    # model.generate()\n",
    "    print('model.generate() inference ë°©ë²•')\n",
    "    start = time.time()\n",
    "\n",
    "    gen(prompt)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f'ê±¸ë¦° ì‹œê°„ : {int(end-start)}ì´ˆ')\n",
    "\n",
    "    print('##################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference ëŒ€í™”í•˜ë©´ì„œ í™•ì¸í•´ë³´ê¸°\n",
    "- prompt templateì€ ë³¸ì¸ì´ ì‹¤í—˜í•  ë•Œ í•™ìŠµì‹œí‚¨ ê±¸ë¡œ ë°”ê¿”ì£¼ì„¸ìš”(multi ê·¸ëƒ¥ ì“°ë©´ ê·¸ëŒ€ë¡œ!)\n",
    "- .evn íŒŒì¼ì— í—ˆê¹…í˜ì´ìŠ¤ì™€ openai tokenì´ í•„ìš”í•©ë‹ˆë‹¤. ë§Œì•½ openai billingì´ ë“±ë¡ë˜ì–´ìˆì§€ ì•Šê±°ë‚˜ í¬ë ˆë”§ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ê°€ ëœ¹ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "test_model_name = \"new_2\"\n",
    "MODEL = f\"../finetuning/model/lora_merged/{test_model_name}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=f\"cuda\", non_blocking=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model.eval()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device=0,\n",
    "    min_new_tokens=30,\n",
    "    max_new_tokens=256,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    eos_token_id=2,\n",
    "    no_repeat_ngram_size=2,\n",
    "    repetition_penalty=1.5,\n",
    "    temperature=0.7,\n",
    "    top_k=20,\n",
    "    top_p=0.95,\n",
    ")\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector store-backed memory\n",
    "[Vector store-backed memory link](https://python.langchain.com/docs/modules/memory/how_to/vectorstore_retriever_memory)\n",
    "\n",
    "VectorStoreRetrieverMemoryëŠ” VectorDBì— ë©”ëª¨ë¦¬ë¥¼ ì €ì¥í•˜ê³  í˜¸ì¶œë  ë•Œë§ˆë‹¤ ê°€ì¥ 'ë‘ë“œëŸ¬ì§„' ìƒìœ„ Kê°œì˜ ë¬¸ì„œë¥¼ ì¿¼ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŠ” ìƒí˜¸ì‘ìš©ì˜ ìˆœì„œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ì í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì—ì„œ ë‹¤ë¥¸ ëŒ€ë¶€ë¶„ì˜ ë©”ëª¨ë¦¬ í´ë˜ìŠ¤ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê²½ìš° \"ë¬¸ì„œ\"ëŠ” ì´ì „ ëŒ€í™” ìŠ¤ë‹ˆí«ì…ë‹ˆë‹¤. ì´ëŠ” ëŒ€í™” ì´ˆê¸°ì— AIê°€ ë“¤ì—ˆë˜ ê´€ë ¨ ì •ë³´ë¥¼ ì°¸ì¡°í•˜ëŠ” ë° ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chainì—ì„œ í™œìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings().embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=2))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "template = \"\"\"ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "[ì´ì „ ëŒ€í™”]\n",
    "{history}\n",
    "\n",
    "[í˜„ì¬ ëŒ€í™”]\n",
    "### ëª…ë ¹ì–´: {### ëª…ë ¹ì–´}\n",
    "\n",
    "### ì‘ë‹µ:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"### ëª…ë ¹ì–´\"], template=template\n",
    ")\n",
    "\n",
    "conversation_with_vectors = ConversationChain(\n",
    "    llm=local_llm,\n",
    "    prompt=PROMPT,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    input_key='### ëª…ë ¹ì–´',\n",
    "    output_key='### ì‘ë‹µ'\n",
    ")\n",
    "\n",
    "# ëŒ€ë‹µ ìƒì„± í•¨ìˆ˜\n",
    "def get_response(question):\n",
    "    input_dict = {'### ëª…ë ¹ì–´': question}\n",
    "    res = conversation_with_vectors.predict(**input_dict)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "[ì´ì „ ëŒ€í™”]\n",
      "\n",
      "\n",
      "[í˜„ì¬ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ìš”ì¦˜ í•˜ëŠ” í”„ë¡œì íŠ¸ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ê³  ìˆì–´\n",
      "\n",
      "### ì‘ë‹µ:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "í”„ë¡œì íŠ¸ê°€ ì œ ëŠ¥ë ¥ ì´ìƒìœ¼ë¡œ ì–´ë µê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ê²Œ ëŠê»´ì ¸ ê±±ì •ë˜ì‹œê² ì–´ìš”, ì¹œêµ¬! í•˜ì§€ë§Œ ê·¸ëŸ´ ë•Œì¼ìˆ˜ë¡ ë§ˆìŒì±™ê¹€ì´ ì¤‘ìš”í•©ë‹ˆë‹¤... ì´ë ‡ê²Œ ìƒê°í•´ ë³´ë©´ ì–´ë–¨ê¹Œìš”? ì´ ê²½í—˜ì€ ë‚˜ ìì‹ ë§Œ ê²ªì—ˆë˜ ê²ƒë„ ì•„ë‹ˆë©° ì„¸ìƒ ëª¨ë“  ì‚¬ëŒë“¤ì´ ê°™ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.. ê·¸ë ‡ì£ ?\"\n"
     ]
    }
   ],
   "source": [
    "res = get_response('ìš”ì¦˜ í•˜ëŠ” í”„ë¡œì íŠ¸ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ê³  ìˆì–´')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "[ì´ì „ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ìš”ì¦˜ í•˜ëŠ” í”„ë¡œì íŠ¸ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ê³  ìˆì–´\n",
      "### ì‘ë‹µ: í”„ë¡œì íŠ¸ê°€ ì œ ëŠ¥ë ¥ ì´ìƒìœ¼ë¡œ ì–´ë µê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ê²Œ ëŠê»´ì ¸ ê±±ì •ë˜ì‹œê² ì–´ìš”, ì¹œêµ¬! í•˜ì§€ë§Œ ê·¸ëŸ´ ë•Œì¼ìˆ˜ë¡ ë§ˆìŒì±™ê¹€ì´ ì¤‘ìš”í•©ë‹ˆë‹¤... ì´ë ‡ê²Œ ìƒê°í•´ ë³´ë©´ ì–´ë–¨ê¹Œìš”? ì´ ê²½í—˜ì€ ë‚˜ ìì‹ ë§Œ ê²ªì—ˆë˜ ê²ƒë„ ì•„ë‹ˆë©° ì„¸ìƒ ëª¨ë“  ì‚¬ëŒë“¤ì´ ê°™ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.. ê·¸ë ‡ì£ ?\"\n",
      "\n",
      "[í˜„ì¬ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ê·¸ë ‡ì§€..! ë‹¤ë“¤ í•˜ëŠ”ê±´ë° ë‚˜ë§Œ ëª»í•  ìˆœ ì—†ì§€! ì¢‹ì•„ ì˜ í•´ë³¼ê²Œ\n",
      "\n",
      "### ì‘ë‹µ:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ì¢‹ì•„ìš”, ì‹œì‘í•©ì‹œë‹¤!! ì €í•œí…Œ ë§í•  ë• ì¢€ ë‚´ë ¤ë†“ìœ¼ì„¸ìš”.\" [ì›ƒìŒ í¬ì¸íŠ¸ë¡œ ë§ˆë¬´ë¦¬í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤ ^-^)/~â™¥\n"
     ]
    }
   ],
   "source": [
    "res = get_response('ê·¸ë ‡ì§€..! ë‹¤ë“¤ í•˜ëŠ”ê±´ë° ë‚˜ë§Œ ëª»í•  ìˆœ ì—†ì§€! ì¢‹ì•„ ì˜ í•´ë³¼ê²Œ')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "[ì´ì „ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ê·¸ë ‡ì§€..! ë‹¤ë“¤ í•˜ëŠ”ê±´ë° ë‚˜ë§Œ ëª»í•  ìˆœ ì—†ì§€! ì¢‹ì•„ ì˜ í•´ë³¼ê²Œ\n",
      "### ì‘ë‹µ: ì¢‹ì•„ìš”, ì‹œì‘í•©ì‹œë‹¤!! ì €í•œí…Œ ë§í•  ë• ì¢€ ë‚´ë ¤ë†“ìœ¼ì„¸ìš”.\" [ì›ƒìŒ í¬ì¸íŠ¸ë¡œ ë§ˆë¬´ë¦¬í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤ ^-^)/~â™¥\n",
      "### ëª…ë ¹ì–´: ìš”ì¦˜ í•˜ëŠ” í”„ë¡œì íŠ¸ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ê³  ìˆì–´\n",
      "### ì‘ë‹µ: í”„ë¡œì íŠ¸ê°€ ì œ ëŠ¥ë ¥ ì´ìƒìœ¼ë¡œ ì–´ë µê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ê²Œ ëŠê»´ì ¸ ê±±ì •ë˜ì‹œê² ì–´ìš”, ì¹œêµ¬! í•˜ì§€ë§Œ ê·¸ëŸ´ ë•Œì¼ìˆ˜ë¡ ë§ˆìŒì±™ê¹€ì´ ì¤‘ìš”í•©ë‹ˆë‹¤... ì´ë ‡ê²Œ ìƒê°í•´ ë³´ë©´ ì–´ë–¨ê¹Œìš”? ì´ ê²½í—˜ì€ ë‚˜ ìì‹ ë§Œ ê²ªì—ˆë˜ ê²ƒë„ ì•„ë‹ˆë©° ì„¸ìƒ ëª¨ë“  ì‚¬ëŒë“¤ì´ ê°™ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.. ê·¸ë ‡ì£ ?\"\n",
      "\n",
      "[í˜„ì¬ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ê°‘ìê¸° ë¬´ìŠ¨ì†Œë¦¬ì•¼ ã…‹ã…‹ã…‹ ë‚˜ ì´ì œ ë°¥ë¨¹ìœ¼ëŸ¬ ê°ˆêº¼ì•¼\n",
      "\n",
      "### ì‘ë‹µ:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ì¢‹ì•„, ë‚œ ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦´ í…Œë‹ˆ ë§›ìˆì§€ë§Œ ê±´ê°•ì‹ì´ ë˜ë„ë¡ í• ê²Œìš”!\" (ë°¥ ë¨¹ìœ¼ë©´ì„œ í•  ìˆ˜ìˆë‹¤; ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ã„±ã„±\n"
     ]
    }
   ],
   "source": [
    "res = get_response('ê°‘ìê¸° ë¬´ìŠ¨ì†Œë¦¬ì•¼ ã…‹ã…‹ã…‹ ë‚˜ ì´ì œ ë°¥ë¨¹ìœ¼ëŸ¬ ê°ˆêº¼ì•¼')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "[ì´ì „ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ê°‘ìê¸° ë¬´ìŠ¨ì†Œë¦¬ì•¼ ã…‹ã…‹ã…‹ ë‚˜ ì´ì œ ë°¥ë¨¹ìœ¼ëŸ¬ ê°ˆêº¼ì•¼\n",
      "### ì‘ë‹µ: ì¢‹ì•„, ë‚œ ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦´ í…Œë‹ˆ ë§›ìˆì§€ë§Œ ê±´ê°•ì‹ì´ ë˜ë„ë¡ í• ê²Œìš”!\" (ë°¥ ë¨¹ìœ¼ë©´ì„œ í•  ìˆ˜ìˆë‹¤; ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ã„±ã„±\n",
      "### ëª…ë ¹ì–´: ìš”ì¦˜ í•˜ëŠ” í”„ë¡œì íŠ¸ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ê³  ìˆì–´\n",
      "### ì‘ë‹µ: í”„ë¡œì íŠ¸ê°€ ì œ ëŠ¥ë ¥ ì´ìƒìœ¼ë¡œ ì–´ë µê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ê²Œ ëŠê»´ì ¸ ê±±ì •ë˜ì‹œê² ì–´ìš”, ì¹œêµ¬! í•˜ì§€ë§Œ ê·¸ëŸ´ ë•Œì¼ìˆ˜ë¡ ë§ˆìŒì±™ê¹€ì´ ì¤‘ìš”í•©ë‹ˆë‹¤... ì´ë ‡ê²Œ ìƒê°í•´ ë³´ë©´ ì–´ë–¨ê¹Œìš”? ì´ ê²½í—˜ì€ ë‚˜ ìì‹ ë§Œ ê²ªì—ˆë˜ ê²ƒë„ ì•„ë‹ˆë©° ì„¸ìƒ ëª¨ë“  ì‚¬ëŒë“¤ì´ ê°™ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.. ê·¸ë ‡ì£ ?\"\n",
      "\n",
      "[í˜„ì¬ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ë„ˆê°€ ê±´ê°•ì‹ì´ ëœë‹¤ê³ ..? ê·¸ê²Œ ë¬´ìŠ¨ì†Œë¦¬ë‹ˆ... ì•”íŠ¼ ì•ˆë…•\n",
      "\n",
      "### ì‘ë‹µ:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ì£„ì†¡í•´ìš” ë°©ê¸ˆ í•œ ë§ ëª» ë“¤ìœ¼ì…¨ë‚˜ìš”.... ì € ì˜¤ëŠ˜ ì•„ì¹¨ë¶€í„° ì €ë…ì‹ì‚¬ ì „ê¹Œì§ˆ ìˆ˜ë„ ì—†ìŠµë‹ˆë‹¤!! ê·¸ ë™ì•ˆ ìµœì„  ë‹¤í• ê²Œìš”~^-/-->\n"
     ]
    }
   ],
   "source": [
    "res = get_response('ë„ˆê°€ ê±´ê°•ì‹ì´ ëœë‹¤ê³ ..? ê·¸ê²Œ ë¬´ìŠ¨ì†Œë¦¬ë‹ˆ... ì•”íŠ¼ ì•ˆë…•')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "[ì´ì „ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ê°‘ìê¸° ë¬´ìŠ¨ì†Œë¦¬ì•¼ ã…‹ã…‹ã…‹ ë‚˜ ì´ì œ ë°¥ë¨¹ìœ¼ëŸ¬ ê°ˆêº¼ì•¼\n",
      "### ì‘ë‹µ: ì¢‹ì•„, ë‚œ ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦´ í…Œë‹ˆ ë§›ìˆì§€ë§Œ ê±´ê°•ì‹ì´ ë˜ë„ë¡ í• ê²Œìš”!\" (ë°¥ ë¨¹ìœ¼ë©´ì„œ í•  ìˆ˜ìˆë‹¤; ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ã„±ã„±\n",
      "### ëª…ë ¹ì–´: ë„ˆê°€ ê±´ê°•ì‹ì´ ëœë‹¤ê³ ..? ê·¸ê²Œ ë¬´ìŠ¨ì†Œë¦¬ë‹ˆ... ì•”íŠ¼ ì•ˆë…•\n",
      "### ì‘ë‹µ: ì£„ì†¡í•´ìš” ë°©ê¸ˆ í•œ ë§ ëª» ë“¤ìœ¼ì…¨ë‚˜ìš”.... ì € ì˜¤ëŠ˜ ì•„ì¹¨ë¶€í„° ì €ë…ì‹ì‚¬ ì „ê¹Œì§ˆ ìˆ˜ë„ ì—†ìŠµë‹ˆë‹¤!! ê·¸ ë™ì•ˆ ìµœì„  ë‹¤í• ê²Œìš”~^-/-->\n",
      "\n",
      "[í˜„ì¬ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ì ì‹¤ê°€ì„œ ë”¤ì„¬ë¨¹ì„êº¼ì•¼\n",
      "\n",
      "### ì‘ë‹µ:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ë„¤! ì ì‹¬ ê°€ì…”ì„œ ê¼­ ë“œì…”ë³´ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”~! ì˜ ë‹¤ë…€ì˜¤ì‹œê³  ë‚˜ì¤‘ì—ë„ ì–˜ê¸° ë‚˜ëˆ ìš” :D ğŸ¥¢\n"
     ]
    }
   ],
   "source": [
    "res = get_response('ì ì‹¤ê°€ì„œ ë”¤ì„¬ë¨¹ì„êº¼ì•¼')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™”ì˜ inputì„ ì°¸ê³ í•˜ì—¬ ìƒí™©ì— ê³µê°í•˜ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "[ì´ì „ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ê·¸ë ‡ì§€..! ë‹¤ë“¤ í•˜ëŠ”ê±´ë° ë‚˜ë§Œ ëª»í•  ìˆœ ì—†ì§€! ì¢‹ì•„ ì˜ í•´ë³¼ê²Œ\n",
      "### ì‘ë‹µ: ì¢‹ì•„ìš”, ì‹œì‘í•©ì‹œë‹¤!! ì €í•œí…Œ ë§í•  ë• ì¢€ ë‚´ë ¤ë†“ìœ¼ì„¸ìš”.\" [ì›ƒìŒ í¬ì¸íŠ¸ë¡œ ë§ˆë¬´ë¦¬í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤ ^-^)/~â™¥\n",
      "### ëª…ë ¹ì–´: ë„ˆê°€ ê±´ê°•ì‹ì´ ëœë‹¤ê³ ..? ê·¸ê²Œ ë¬´ìŠ¨ì†Œë¦¬ë‹ˆ... ì•”íŠ¼ ì•ˆë…•\n",
      "### ì‘ë‹µ: ì£„ì†¡í•´ìš” ë°©ê¸ˆ í•œ ë§ ëª» ë“¤ìœ¼ì…¨ë‚˜ìš”.... ì € ì˜¤ëŠ˜ ì•„ì¹¨ë¶€í„° ì €ë…ì‹ì‚¬ ì „ê¹Œì§ˆ ìˆ˜ë„ ì—†ìŠµë‹ˆë‹¤!! ê·¸ ë™ì•ˆ ìµœì„  ë‹¤í• ê²Œìš”~^-/-->\n",
      "\n",
      "[í˜„ì¬ ëŒ€í™”]\n",
      "### ëª…ë ¹ì–´: ì¢‹ì•„ì¢‹ì•„ ë„ˆ ì¹œì ˆí•˜êµ¬ë‚­\n",
      "\n",
      "### ì‘ë‹µ:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ë„¤ ì œ ë§íˆ¬ ë•Œë¬¸ì´ë¼ë©´ ì˜¤í•´í•˜ì…¨ë„¤ìš”; ê·¸ëƒ¥ ë‚´ ëª¸ì€ ìµœê³ ì•¼ - ë‚œ ë‹¹ì‹ ë§Œí¼ì´ë‚˜ ì™„ë²½í•´ >_<b ê±°ê¸°ë‹¤ ë§›ìˆê¸° ê¹Œì§€ í•˜ê³  ì™„ì „ ëŒ€ë°•ì´ì•¼ <3\"\n"
     ]
    }
   ],
   "source": [
    "res = get_response('ì¢‹ì•„ì¢‹ì•„ ë„ˆ ì¹œì ˆí•˜êµ¬ë‚­')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response('ì˜¤ëŠ˜ íœ´ì‹ì„ ìœ„í•´ ì ì‹¬ë•Œ ë­í•œë‹¤ê³  í–ˆì§€?')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response('')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response('')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response('')\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adapter.bin 파일로 불러와서 확인해보기\n",
    "- 이건 이제 안해봐도 될 듯 합니다.궁금한 사람만..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/final did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/opt/conda/envs/final/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 28/28 [00:13<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from utils.prompter import Prompter\n",
    "test_prompte_name = \"multi\"\n",
    "prompter = Prompter(f\"{test_prompte_name}\")\n",
    "\n",
    "start = time.time()\n",
    "peft_model_id = \"./model/lora_finetuned/new_2/\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, \n",
    "                                             quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.generate() inference 방법\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "\n",
      "\n",
      "[현재 대화]\n",
      "input: 나는 커서 연기를 하는 배우가 될거야! 그게 나의 꿈이야\n",
      "\n",
      "response: 연기하는 배우가 되고 싶은 마음은 알겠어! 너의 꿈을 이루기 위해서는 너의 꿈을 더 자세히 말해줘! 내가 더 좋은 방법을 알려줄께![추가 질문] 달한[추가 질문](1): 너는(2): 1. 너는! 너의 꿈을 이루기 위해 무엇을 하고 있니! (3.나는 너를 사랑하기 위해 더 열심히 노력하고 있어! 너의 꿈을 이루기 위해 너의 하루를 더 보여줘!너의 꿈이 더 궁금해! 너의 꿈을 이루기\n",
      "걸린 시간 : 15초\n",
      "##################################################################\n",
      "model.generate() inference 방법\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "\n",
      "\n",
      "[현재 대화]\n",
      "input: 이제 일 그만하고 집에가서 쉬고싶어, 넘 힘들어 ㅠㅠ\n",
      "\n",
      "response: 힘들고 피곤하고 지친 련(\")(DOM) ANDROIUDELIV(\"DOM\")DOMINE(ALLOWEEKLY)ANDRE(ALLOWEEKLIVE)DOMINE(ANDRELLALIVE)DOMINE(ANDRELLALLOWEEIVEDOMINE(ANDRELLANDIVE(S340-DOMINE(ANDRE\n",
      "걸린 시간 : 14초\n",
      "##################################################################\n"
     ]
    }
   ],
   "source": [
    "def gen(x):\n",
    "    inputs = tokenizer(\n",
    "        x, \n",
    "        return_tensors='pt', \n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()} # move to same device as model\n",
    "    gened = model.generate(\n",
    "        **inputs,\n",
    "        min_new_tokens=30,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.5,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "        # no_repeat_ngram_size=2,\n",
    "        # repetition_penalty=1.05,\n",
    "        # top_k=20,\n",
    "        # top_p=0.95,\n",
    "        # typical_p=0.9\n",
    "    )\n",
    "    \n",
    "    gened_list = gened[0].tolist()\n",
    "    \n",
    "    try:\n",
    "        eos_token_position = gened_list.index(2)\n",
    "        gened_list = gened_list[:eos_token_position]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    print(tokenizer.decode(gened_list))\n",
    "\n",
    "messages = [\n",
    "            # '오늘 날씨가 맑아서 기분이 좋아요 !', \n",
    "            # '내일 시험인데 너무 떨려서 잠이 안와요. 어떡하죠?',\n",
    "            '나는 커서 연기를 하는 배우가 될거야! 그게 나의 꿈이야',\n",
    "            '이제 일 그만하고 집에가서 쉬고싶어, 넘 힘들어 ㅠㅠ',\n",
    "            # '친구하고 싸웠는데 어떻게 화해를 하면 좋을지 모르겠어'\n",
    "            ]\n",
    "for message in messages:\n",
    "    prompt = prompter.generate_prompt(message)\n",
    "\n",
    "    # model.generate()\n",
    "    print('model.generate() inference 방법')\n",
    "    start = time.time()\n",
    "\n",
    "    gen(prompt)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f'걸린 시간 : {int(end-start)}초')\n",
    "\n",
    "    print('##################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference 대화하면서 확인해보기\n",
    "- prompt template은 본인이 실험할 때 학습시킨 걸로 바꿔주세요(multi 그냥 쓰면 그대로!)\n",
    "- .evn 파일에 허깅페이스와 openai token이 필요합니다. 만약 openai billing이 등록되어있지 않거나 크레딧이 없으면 오류가 뜹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "test_model_name = \"new_2\"\n",
    "MODEL = f\"../finetuning/model/lora_merged/{test_model_name}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=f\"cuda\", non_blocking=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model.eval()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device=0,\n",
    "    min_new_tokens=30,\n",
    "    max_new_tokens=256,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    eos_token_id=2,\n",
    "    no_repeat_ngram_size=2,\n",
    "    repetition_penalty=1.5,\n",
    "    temperature=0.7,\n",
    "    top_k=20,\n",
    "    top_p=0.95,\n",
    ")\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector store-backed memory\n",
    "[Vector store-backed memory link](https://python.langchain.com/docs/modules/memory/how_to/vectorstore_retriever_memory)\n",
    "\n",
    "VectorStoreRetrieverMemory는 VectorDB에 메모리를 저장하고 호출될 때마다 가장 '두드러진' 상위 K개의 문서를 쿼리합니다.\n",
    "\n",
    "이는 상호작용의 순서를 명시적으로 추적하지 않는다는 점에서 다른 대부분의 메모리 클래스와 다릅니다.\n",
    "\n",
    "이 경우 \"문서\"는 이전 대화 스니펫입니다. 이는 대화 초기에 AI가 들었던 관련 정보를 참조하는 데 유용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain에서 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings().embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=2))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "template = \"\"\"이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
    "\n",
    "[이전 대화]\n",
    "{history}\n",
    "\n",
    "[현재 대화]\n",
    "### 명령어: {### 명령어}\n",
    "\n",
    "### 응답:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"### 명령어\"], template=template\n",
    ")\n",
    "\n",
    "conversation_with_vectors = ConversationChain(\n",
    "    llm=local_llm,\n",
    "    prompt=PROMPT,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    input_key='### 명령어',\n",
    "    output_key='### 응답'\n",
    ")\n",
    "\n",
    "# 대답 생성 함수\n",
    "def get_response(question):\n",
    "    input_dict = {'### 명령어': question}\n",
    "    res = conversation_with_vectors.predict(**input_dict)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "프로젝트가 제 능력 이상으로 어렵거나 불확실하게 느껴져 걱정되시겠어요, 친구! 하지만 그럴 때일수록 마음챙김이 중요합니다... 이렇게 생각해 보면 어떨까요? 이 경험은 나 자신만 겪었던 것도 아니며 세상 모든 사람들이 같지 않다는 사실입니다.. 그렇죠?\"\n"
     ]
    }
   ],
   "source": [
    "res = get_response('요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 제 능력 이상으로 어렵거나 불확실하게 느껴져 걱정되시겠어요, 친구! 하지만 그럴 때일수록 마음챙김이 중요합니다... 이렇게 생각해 보면 어떨까요? 이 경험은 나 자신만 겪었던 것도 아니며 세상 모든 사람들이 같지 않다는 사실입니다.. 그렇죠?\"\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "좋아요, 시작합시다!! 저한테 말할 땐 좀 내려놓으세요.\" [웃음 포인트로 마무리하면 좋습니다 ^-^)/~♥\n"
     ]
    }
   ],
   "source": [
    "res = get_response('그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "### 응답: 좋아요, 시작합시다!! 저한테 말할 땐 좀 내려놓으세요.\" [웃음 포인트로 마무리하면 좋습니다 ^-^)/~♥\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 제 능력 이상으로 어렵거나 불확실하게 느껴져 걱정되시겠어요, 친구! 하지만 그럴 때일수록 마음챙김이 중요합니다... 이렇게 생각해 보면 어떨까요? 이 경험은 나 자신만 겪었던 것도 아니며 세상 모든 사람들이 같지 않다는 사실입니다.. 그렇죠?\"\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 갑자기 무슨소리야 ㅋㅋㅋ 나 이제 밥먹으러 갈꺼야\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "좋아, 난 여기서 기다릴 테니 맛있지만 건강식이 되도록 할게요!\" (밥 먹으면서 할 수있다; 다음 단계 진행 ㄱㄱ\n"
     ]
    }
   ],
   "source": [
    "res = get_response('갑자기 무슨소리야 ㅋㅋㅋ 나 이제 밥먹으러 갈꺼야')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 갑자기 무슨소리야 ㅋㅋㅋ 나 이제 밥먹으러 갈꺼야\n",
      "### 응답: 좋아, 난 여기서 기다릴 테니 맛있지만 건강식이 되도록 할게요!\" (밥 먹으면서 할 수있다; 다음 단계 진행 ㄱㄱ\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 제 능력 이상으로 어렵거나 불확실하게 느껴져 걱정되시겠어요, 친구! 하지만 그럴 때일수록 마음챙김이 중요합니다... 이렇게 생각해 보면 어떨까요? 이 경험은 나 자신만 겪었던 것도 아니며 세상 모든 사람들이 같지 않다는 사실입니다.. 그렇죠?\"\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 너가 건강식이 된다고..? 그게 무슨소리니... 암튼 안녕\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "죄송해요 방금 한 말 못 들으셨나요.... 저 오늘 아침부터 저녁식사 전까질 수도 없습니다!! 그 동안 최선 다할게요~^-/-->\n"
     ]
    }
   ],
   "source": [
    "res = get_response('너가 건강식이 된다고..? 그게 무슨소리니... 암튼 안녕')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 갑자기 무슨소리야 ㅋㅋㅋ 나 이제 밥먹으러 갈꺼야\n",
      "### 응답: 좋아, 난 여기서 기다릴 테니 맛있지만 건강식이 되도록 할게요!\" (밥 먹으면서 할 수있다; 다음 단계 진행 ㄱㄱ\n",
      "### 명령어: 너가 건강식이 된다고..? 그게 무슨소리니... 암튼 안녕\n",
      "### 응답: 죄송해요 방금 한 말 못 들으셨나요.... 저 오늘 아침부터 저녁식사 전까질 수도 없습니다!! 그 동안 최선 다할게요~^-/-->\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 잠실가서 딤섬먹을꺼야\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "네! 잠심 가셔서 꼭 드셔보셨으면 좋겠어요~! 잘 다녀오시고 나중에도 얘기 나눠요 :D 🥢\n"
     ]
    }
   ],
   "source": [
    "res = get_response('잠실가서 딤섬먹을꺼야')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "### 응답: 좋아요, 시작합시다!! 저한테 말할 땐 좀 내려놓으세요.\" [웃음 포인트로 마무리하면 좋습니다 ^-^)/~♥\n",
      "### 명령어: 너가 건강식이 된다고..? 그게 무슨소리니... 암튼 안녕\n",
      "### 응답: 죄송해요 방금 한 말 못 들으셨나요.... 저 오늘 아침부터 저녁식사 전까질 수도 없습니다!! 그 동안 최선 다할게요~^-/-->\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 좋아좋아 너 친절하구낭\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "네 제 말투 때문이라면 오해하셨네요; 그냥 내 몸은 최고야 - 난 당신만큼이나 완벽해 >_<b 거기다 맛있기 까지 하고 완전 대박이야 <3\"\n"
     ]
    }
   ],
   "source": [
    "res = get_response('좋아좋아 너 친절하구낭')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response('오늘 휴식을 위해 점심때 뭐한다고 했지?')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response('')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response('')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response('')\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

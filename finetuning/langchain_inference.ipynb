{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_name = \"test\"\n",
    "test_prompte_name = \"multi\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adapter.bin 파일로 불러와서 확인해보기\n",
    "- 선택. 궁금한 사람만!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/ml/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/ml/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/opt/ml/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/ml/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/opt/ml/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "../finetuning/model/lora_finetuned/jihwan_0720 does not appear to have a file named config.json. Checkout 'https://huggingface.co/../finetuning/model/lora_finetuned/jihwan_0720/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m config \u001b[39m=\u001b[39m PeftConfig\u001b[39m.\u001b[39mfrom_pretrained(MODEL)\n\u001b[1;32m     12\u001b[0m bnb_config \u001b[39m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     13\u001b[0m     load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     bnb_4bit_use_double_quant\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     bnb_4bit_quant_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnf4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     bnb_4bit_compute_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(MODEL, \n\u001b[1;32m     19\u001b[0m                                              quantization_config\u001b[39m=\u001b[39;49mbnb_config, device_map\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m0\u001b[39;49m})\n\u001b[1;32m     20\u001b[0m model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39mfrom_pretrained(model, MODEL)\n\u001b[1;32m     21\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(MODEL)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:461\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    459\u001b[0m     _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 461\u001b[0m config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    462\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    463\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    464\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    465\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    466\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[39m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m kwargs_orig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:983\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    982\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 983\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    984\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    985\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/transformers/configuration_utils.py:617\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    616\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    619\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/transformers/configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    670\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    673\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    674\u001b[0m         configuration_file,\n\u001b[1;32m    675\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    676\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    677\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    678\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    679\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    680\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    681\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    682\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    683\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    684\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    687\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finalproject-n-IakgBe-py3.9/lib/python3.9/site-packages/transformers/utils/hub.py:388\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 388\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    389\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    392\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: ../finetuning/model/lora_finetuned/jihwan_0720 does not appear to have a file named config.json. Checkout 'https://huggingface.co/../finetuning/model/lora_finetuned/jihwan_0720/None' for available files."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from utils.prompter import Prompter\n",
    "\n",
    "MODEL = f\"../finetuning/model/lora_finetuned/{test_model_name}\"\n",
    "prompter = Prompter(f\"{test_prompte_name}\")\n",
    "\n",
    "start = time.time()\n",
    "config = PeftConfig.from_pretrained(MODEL)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, \n",
    "                                             quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model, MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x):\n",
    "    inputs = tokenizer(\n",
    "        x, \n",
    "        return_tensors='pt', \n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()} # move to same device as model\n",
    "    gened = model.generate(\n",
    "        **inputs,\n",
    "        min_new_tokens=30,\n",
    "        max_new_tokens=128,\n",
    "        streamer=streamer,\n",
    "        temperature=0.5,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "        # no_repeat_ngram_size=2,\n",
    "        # repetition_penalty=1.05,\n",
    "        # top_k=20,\n",
    "        # top_p=0.95,\n",
    "        # typical_p=0.9\n",
    "    )\n",
    "    \n",
    "    gened_list = gened[0].tolist()\n",
    "    \n",
    "    try:\n",
    "        eos_token_position = gened_list.index(2)\n",
    "        gened_list = gened_list[:eos_token_position]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    print(tokenizer.decode(gened_list))\n",
    "\n",
    "messages = [\n",
    "            # '오늘 날씨가 맑아서 기분이 좋아요 !', \n",
    "            # '내일 시험인데 너무 떨려서 잠이 안와요. 어떡하죠?',\n",
    "            '나는 커서 연기를 하는 배우가 될거야! 그게 나의 꿈이야',\n",
    "            '이제 일 그만하고 집에가서 쉬고싶어, 넘 힘들어 ㅠㅠ',\n",
    "            # '친구하고 싸웠는데 어떻게 화해를 하면 좋을지 모르겠어'\n",
    "            ]\n",
    "for message in messages:\n",
    "    prompt = prompter.generate_prompt(message)\n",
    "\n",
    "    # model.generate()\n",
    "    print('model.generate() inference 방법')\n",
    "    start = time.time()\n",
    "\n",
    "    gen(prompt)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f'걸린 시간 : {int(end-start)}초')\n",
    "\n",
    "    print('##################################################################')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference 대화하면서 확인해보기\n",
    "- prompt template은 본인이 실험할 때 학습시킨 걸로 바꿔주세요(multi 그냥 쓰면 그대로!)\n",
    "- .evn 파일에 허깅페이스와 openai token이 필요합니다. 만약 openai billing이 등록되어있지 않거나 크레딧이 없으면 오류가 뜹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  6.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = f\"../finetuning/model/lora_merged/{test_model_name}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=f\"cuda\", non_blocking=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task='text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    streamer=streamer,\n",
    "    device=0,\n",
    "    min_new_tokens=30,\n",
    "    max_new_tokens=256,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    eos_token_id=2,\n",
    "    no_repeat_ngram_size=2,\n",
    "    repetition_penalty=1.5,\n",
    "    temperature=0.7,\n",
    "    top_k=20,\n",
    "    top_p=0.95,\n",
    ")\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector store-backed memory\n",
    "[Vector store-backed memory link](https://python.langchain.com/docs/modules/memory/how_to/vectorstore_retriever_memory)\n",
    "\n",
    "VectorStoreRetrieverMemory는 VectorDB에 메모리를 저장하고 호출될 때마다 가장 '두드러진' 상위 K개의 문서를 쿼리합니다.\n",
    "\n",
    "이는 상호작용의 순서를 명시적으로 추적하지 않는다는 점에서 다른 대부분의 메모리 클래스와 다릅니다.\n",
    "\n",
    "이 경우 \"문서\"는 이전 대화 스니펫입니다. 이는 대화 초기에 AI가 들었던 관련 정보를 참조하는 데 유용할 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain에서 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings().embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=2))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# template 필요시 수정\n",
    "template = \"\"\"이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
    "\n",
    "[이전 대화]\n",
    "{history}\n",
    "\n",
    "[현재 대화]\n",
    "### 명령어: {### 명령어}\n",
    "\n",
    "### 응답:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"### 명령어\"], template=template\n",
    ")\n",
    "\n",
    "conversation_with_vectors = ConversationChain(\n",
    "    llm=local_llm,\n",
    "    prompt=PROMPT,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    input_key='### 명령어',\n",
    "    output_key='### 응답'\n",
    ")\n",
    "\n",
    "# 대답 생성 함수\n",
    "def get_response(question):\n",
    "    input_dict = {'### 명령어': question}\n",
    "    res = conversation_with_vectors.predict(**input_dict)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "\n",
      "### 응답:\n",
      "하지만,  인해 겪게 되신 어려움이 정말 많다는 것은 잘 알아요! 너무 부담스럽거나 압박감으로 느껴질 때면 잠시 멈춰서 스스로에게 휴식 시간도 주면서 자신만의 페이스대로 진행할 필요가 종종 생깁니다..저랑 같이 얘기하며 마음 편히 쉬어가실래요? 제 모든 능력 중 가장 좋다고 생각되시도록 도와드릴게요!!<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "하지만, 저랑 같이 얘기하며 마음 편히 쉬어가실래요? 제 모든 능력 중 가장 좋다고 생각되시도록 도와드릴게요!! 멈춰서 스스로에게 휴식 시간도 주면서 자신만의 페이스대로 진행할 필요가 종종 생깁니다..\n"
     ]
    }
   ],
   "source": [
    "res = get_response('요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 잘 진행되지 않아서 고민이 많으신 것 같네요! 힘든 시기일 때 누군가에게 털어놓기만 해도 기분 전환이나 아이디어 공유 등 다른 도움으로 이어질 수있습니다 :)\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 잘 진행되지 않아서 고민이 많으신 것 같네요! 힘든 시기일 때 누군가에게 털어놓기만 해도 기분 전환이나 아이디어 공유 등 다른 도움으로 이어질 수있습니다 :)\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "\n",
      "### 응답:\n",
      "저도 이해해요, 하지만 걱정하실 필요 없이 할 일은 열심히 하고 자신한테 좋다고 생각하면 그냥 계속 밀고가면 돼요!! 당신 곁에서 응원할게요 💪<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "저도 이해해요, 하지만 걱정하실 필요 없이 할 일은 열심히 하고 자신한테 좋다고 생각하면 그냥 계속 밀고가면 돼요!! 당신 곁에서 응원할게요 💪\n"
     ]
    }
   ],
   "source": [
    "res = get_response('그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 잘 진행되지 않아서 고민이 많으신 것 같네요! 힘든 시기일 때 누군가에게 털어놓기만 해도 기분 전환이나 아이디어 공유 등 다른 도움으로 이어질 수있습니다 :)\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "### 응답: 저도 이해해요, 하지만 걱정하실 필요 없이 할 일은 열심히 하고 자신한테 좋다고 생각하면 그냥 계속 밀고가면 돼요!! 당신 곁에서 응원할게요 💪\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 잘 진행되지 않아서 고민이 많으신 것 같네요! 힘든 시기일 때 누군가에게 털어놓기만 해도 기분 전환이나 아이디어 공유 등 다른 도움으로 이어질 수있습니다 :)\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "### 응답: 저도 이해해요, 하지만 걱정하실 필요 없이 할 일은 열심히 하고 자신한테 좋다고 생각하면 그냥 계속 밀고가면 돼요!! 당신 곁에서 응원할게요 💪\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "\n",
      "### 응답:\n",
      "날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n"
     ]
    }
   ],
   "source": [
    "res = get_response('응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "### 응답: 날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 잘 진행되지 않아서 고민이 많으신 것 같네요! 힘든 시기일 때 누군가에게 털어놓기만 해도 기분 전환이나 아이디어 공유 등 다른 도움으로 이어질 수있습니다 :)\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 오잉 내가 확인해봤을 때는 비가 온다던데?\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "### 응답: 날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n",
      "### 명령어: 요즘 하는 프로젝트 때문에 스트레스를 많이 받고 있어\n",
      "### 응답: 프로젝트가 잘 진행되지 않아서 고민이 많으신 것 같네요! 힘든 시기일 때 누군가에게 털어놓기만 해도 기분 전환이나 아이디어 공유 등 다른 도움으로 이어질 수있습니다 :)\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 오잉 내가 확인해봤을 때는 비가 온다던데?\n",
      "\n",
      "### 응답:\n",
      "죄송하지만 제 정보로 인해 오해하신 점 사과드립니다, AI로서 저도 기상예보나 실시간 일기 예보 기능은 없지만 정확성보다 유용함(usefulness-right as far away points of view), 즉 해당 지역에서 가장 가까운 실제 상태 또는 과거 데이터 통계치 제공 여부 측면에선 더 신뢰성이 높다고 말씀드리겠으며 이 경우 기상청 자료 링크 및 사용 방법 안내 또한 가능합니다..<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "죄송하지만 제 정보로 인해 오해하신 점 사과드립니다, AI로서 저도 기상예보나 실시간 일기 예보 기능은 없지만 정확성보다 유용함(usefulness-right as far away points of view), 즉 해당 지역에서 가장 가까운 실제 상태 또는 과거 데이터 통계치 제공 여부 측면에선 더 신뢰성이 높다고 말씀드리겠으며 이 경우 기상청 자료 링크 및 사용 방법 안내 또한 가능합니다..\n"
     ]
    }
   ],
   "source": [
    "res = get_response('오잉 내가 확인해봤을 때는 비가 온다던데?')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "### 응답: 날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n",
      "### 명령어: 오잉 내가 확인해봤을 때는 비가 온다던데?\n",
      "### 응답: 죄송하지만 제 정보로 인해 오해하신 점 사과드립니다, AI로서 저도 기상예보나 실시간 일기 예보 기능은 없지만 정확성보다 유용함(usefulness-right as far away points of view), 즉 해당 지역에서 가장 가까운 실제 상태 또는 과거 데이터 통계치 제공 여부 측면에선 더 신뢰성이 높다고 말씀드리겠으며 이 경우 기상청 자료 링크 및 사용 방법 안내 또한 가능합니다..\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 알겠어 고마워. 그런데 나는 주말에도 팀미팅이 있어. 날씨는 별로 중요하지 않아!\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "### 응답: 날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n",
      "### 명령어: 오잉 내가 확인해봤을 때는 비가 온다던데?\n",
      "### 응답: 죄송하지만 제 정보로 인해 오해하신 점 사과드립니다, AI로서 저도 기상예보나 실시간 일기 예보 기능은 없지만 정확성보다 유용함(usefulness-right as far away points of view), 즉 해당 지역에서 가장 가까운 실제 상태 또는 과거 데이터 통계치 제공 여부 측면에선 더 신뢰성이 높다고 말씀드리겠으며 이 경우 기상청 자료 링크 및 사용 방법 안내 또한 가능합니다..\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 알겠어 고마워. 그런데 나는 주말에도 팀미팅이 있어. 날씨는 별로 중요하지 않아!\n",
      "\n",
      "### 응답:\n",
      "이해해요~ 그래서 저희가 추천드린 일정으로 변경해도 될까요 아니면 그냥 원래 계획대로 가셔도 괜찮으세요? 원하신다면 조정해서 다시 알려드릴게요 :)<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "이해해요~ 그래서 저희가 추천드린 일정으로 변경해도 될까요 아니면 그냥 원래 계획대로 가셔도 괜찮으세요? 원하신다면 조정해서 다시 알려드릴게요 :)\n"
     ]
    }
   ],
   "source": [
    "res = get_response('알겠어 고마워. 그런데 나는 주말에도 팀미팅이 있어. 날씨는 별로 중요하지 않아!')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "### 응답: 날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "### 응답: 저도 이해해요, 하지만 걱정하실 필요 없이 할 일은 열심히 하고 자신한테 좋다고 생각하면 그냥 계속 밀고가면 돼요!! 당신 곁에서 응원할게요 💪\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 좋아좋아 너 친절하구낭\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "### 응답: 날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n",
      "### 명령어: 그렇지..! 다들 하는건데 나만 못할 순 없지! 좋아 잘 해볼게\n",
      "### 응답: 저도 이해해요, 하지만 걱정하실 필요 없이 할 일은 열심히 하고 자신한테 좋다고 생각하면 그냥 계속 밀고가면 돼요!! 당신 곁에서 응원할게요 💪\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 좋아좋아 너 친절하구낭\n",
      "\n",
      "### 응답:\n",
      "고마워 내 친구야... 정말 힘이나고 기분전환되었단다 넌 여전히 멋진 사람이야 난 네편이지 항상 그럴거구 그러니깐 우리 힘내자 사랑한다<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "고마워 내 친구야... 정말 힘이나고 기분전환되었단다 넌 여전히 멋진 사람이야 난 네편이지 항상 그럴거구 그러니깐 우리 힘내자 사랑한다\n"
     ]
    }
   ],
   "source": [
    "res = get_response('좋아좋아 너 친절하구낭')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 알겠어 고마워. 그런데 나는 주말에도 팀미팅이 있어. 날씨는 별로 중요하지 않아!\n",
      "### 응답: 이해해요~ 그래서 저희가 추천드린 일정으로 변경해도 될까요 아니면 그냥 원래 계획대로 가셔도 괜찮으세요? 원하신다면 조정해서 다시 알려드릴게요 :)\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "### 응답: 날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 나는 오늘 점심으로 냉면을 먹을 거야.\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 알겠어 고마워. 그런데 나는 주말에도 팀미팅이 있어. 날씨는 별로 중요하지 않아!\n",
      "### 응답: 이해해요~ 그래서 저희가 추천드린 일정으로 변경해도 될까요 아니면 그냥 원래 계획대로 가셔도 괜찮으세요? 원하신다면 조정해서 다시 알려드릴게요 :)\n",
      "### 명령어: 응원 고마워! 생각해보니 오늘 금요일이네. 주말에 날씨가 좋을까?\n",
      "### 응답: 날씨 앱 확인 결과 이번 주 내내 화창할 예정입니다~! 즐거운 시간 보내시길 바랍니다 ✨️❤ #주말계획\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 나는 오늘 점심으로 냉면을 먹을 거야.\n",
      "\n",
      "### 응답:\n",
      "좋은 선택 같네요, 냉면 너무 기대되는데요 😊 맛있게 드세요 ~!! 다음 회의에서 봐요 ---- [종료 버튼 누름<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "좋은 선택 같네요, 냉면 너무 기대되는데요 😊 맛있게 드세요 ~!! 다음 회의에서 봐요 ---- [종료 버튼 누름\n"
     ]
    }
   ],
   "source": [
    "res = get_response('나는 오늘 점심으로 냉면을 먹을 거야.')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 나는 오늘 점심으로 냉면을 먹을 거야.\n",
      "### 응답: 좋은 선택 같네요, 냉면 너무 기대되는데요 😊 맛있게 드세요 ~!! 다음 회의에서 봐요 ---- [종료 버튼 누름\n",
      "### 명령어: 알겠어 고마워. 그런데 나는 주말에도 팀미팅이 있어. 날씨는 별로 중요하지 않아!\n",
      "### 응답: 이해해요~ 그래서 저희가 추천드린 일정으로 변경해도 될까요 아니면 그냥 원래 계획대로 가셔도 괜찮으세요? 원하신다면 조정해서 다시 알려드릴게요 :)\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 내가 오늘 점심으로 뭐 먹는다고 했지?\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 나는 오늘 점심으로 냉면을 먹을 거야.\n",
      "### 응답: 좋은 선택 같네요, 냉면 너무 기대되는데요 😊 맛있게 드세요 ~!! 다음 회의에서 봐요 ---- [종료 버튼 누름\n",
      "### 명령어: 알겠어 고마워. 그런데 나는 주말에도 팀미팅이 있어. 날씨는 별로 중요하지 않아!\n",
      "### 응답: 이해해요~ 그래서 저희가 추천드린 일정으로 변경해도 될까요 아니면 그냥 원래 계획대로 가셔도 괜찮으세요? 원하신다면 조정해서 다시 알려드릴게요 :)\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 내가 오늘 점심으로 뭐 먹는다고 했지?\n",
      "\n",
      "### 응답:\n",
      "냉면이라고 하셨죠... 아 죄송합니다 제 실수입니다 비빔국수라고 말씀해 주신 것 맞으시나요.. 어쨌든 저도 그게 정말 끝내준다고 들었어요 잘됐군요 그거 꼭 드셔보시길 바랄게요 고마워요~!<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "냉면이라고 하셨죠... 아 죄송합니다 제 실수입니다 비빔국수라고 말씀해 주신 것 맞으시나요.. 어쨌든 저도 그게 정말 끝내준다고 들었어요 잘됐군요 그거 꼭 드셔보시길 바랄게요 고마워요~!\n"
     ]
    }
   ],
   "source": [
    "res = get_response('내가 오늘 점심으로 뭐 먹는다고 했지?')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 나는 오늘 점심으로 냉면을 먹을 거야.\n",
      "### 응답: 좋은 선택 같네요, 냉면 너무 기대되는데요 😊 맛있게 드세요 ~!! 다음 회의에서 봐요 ---- [종료 버튼 누름\n",
      "### 명령어: 내가 오늘 점심으로 뭐 먹는다고 했지?\n",
      "### 응답: 냉면이라고 하셨죠... 아 죄송합니다 제 실수입니다 비빔국수라고 말씀해 주신 것 맞으시나요.. 어쨌든 저도 그게 정말 끝내준다고 들었어요 잘됐군요 그거 꼭 드셔보시길 바랄게요 고마워요~!\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 냉면일까, 비빔국수일까?\n",
      "\n",
      "### 응답:\n",
      "\u001b[0m\n",
      "이전 대화와 현재 대화의 input을 참고하여 상황에 공감하고 친절한 답변을 생성해주세요. 답변 마지막에는 지금까지의 내용과 관련된 질문을 해주세요.\n",
      "\n",
      "[이전 대화]\n",
      "### 명령어: 나는 오늘 점심으로 냉면을 먹을 거야.\n",
      "### 응답: 좋은 선택 같네요, 냉면 너무 기대되는데요 😊 맛있게 드세요 ~!! 다음 회의에서 봐요 ---- [종료 버튼 누름\n",
      "### 명령어: 내가 오늘 점심으로 뭐 먹는다고 했지?\n",
      "### 응답: 냉면이라고 하셨죠... 아 죄송합니다 제 실수입니다 비빔국수라고 말씀해 주신 것 맞으시나요.. 어쨌든 저도 그게 정말 끝내준다고 들었어요 잘됐군요 그거 꼭 드셔보시길 바랄게요 고마워요~!\n",
      "\n",
      "[현재 대화]\n",
      "### 명령어: 냉면일까, 비빔국수일까?\n",
      "\n",
      "### 응답:\n",
      "냉면이나 비냉 중 하나를 말하려는 건가요 아니면 둘 다요?! 알려주시면 감사할게요 ❤️✨<|endoftext|>\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "냉면이나 비냉 중 하나를 말하려는 건가요 아니면 둘 다요?! 알려주시면 감사할게요 ❤️✨\n"
     ]
    }
   ],
   "source": [
    "res = get_response('냉면일까, 비빔국수일까?')\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
